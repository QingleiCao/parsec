diff --git a/parsec/parsec.c b/parsec/parsec.c
index 9577f51b4..5277e3ca2 100644
--- a/parsec/parsec.c
+++ b/parsec/parsec.c
@@ -360,8 +360,6 @@ static void parsec_vp_init( parsec_vp_t *vp,
     }
 }
 
-static int check_overlapping_binding(parsec_context_t *context);
-
 #define DEFAULT_APP_NAME "app_name"
 
 #define GET_INT_ARGV(CMD, ARGV, VALUE) \
@@ -783,8 +781,6 @@ parsec_context_t* parsec_init( int nb_cores, int* pargc, char** pargv[] )
     /* Introduce communication engine */
     (void)parsec_remote_dep_init(context);
 
-    (void)check_overlapping_binding(context);
-
     PARSEC_PINS_INIT(context);
     if(profiling_enabled && (0 == parsec_pins_nb_modules_enabled())) {
         if(parsec_debug_rank == 0)
@@ -2511,61 +2507,76 @@ int parsec_parse_binding_parameter(const char * option, parsec_context_t* contex
 #endif /* PARSEC_HAVE_HWLOC && PARSEC_HAVE_HWLOC_BITMAP */
 }
 
-static int check_overlapping_binding(parsec_context_t *context) {
+/**
+ * @brief Check that the binding is correct. However, this operation is extremely expensive
+ *        and highly unscalable so we should only do this operation when really necessary.
+ * 
+ * @param context 
+ * @return int SUCCESS if the global bindings are OK, error otherwise.
+ */
+int parsec_check_overlapping_binding(parsec_context_t *context)
+{
+    if( 1024 < context->nb_nodes ) {  /* At some point we need to assume the users are doing the right thing,
+                                       * especially when they run at scale.
+                                       */
+        return PARSEC_SUCCESS;
+    }
 #if defined(DISTRIBUTED) && defined(PARSEC_HAVE_MPI) && defined(PARSEC_HAVE_HWLOC) && defined(PARSEC_HAVE_HWLOC_BITMAP)
-    MPI_Comm comml = MPI_COMM_NULL; int i, nl = 0, rl = MPI_PROC_NULL;
-    MPI_Comm commw = (MPI_Comm)context->comm_ctx;
-    assert(-1 != context->comm_ctx);
-    MPI_Comm_split_type(commw, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comml);
-    MPI_Comm_size(comml, &nl);
-    if( 1 < nl && slow_bind_warning ) {
-        /* Hu-ho, double check that our binding is not conflicting with other
-         * local procs */
-        MPI_Comm_rank(comml, &rl);
-        char *myset = NULL, *allsets = NULL;
-
-        if( 0 != hwloc_bitmap_list_asprintf(&myset, context->cpuset_allowed_mask) ) {
-        }
-        int setlen = strlen(myset);
-        int *setlens = NULL;
-        if( 0 == rl ) {
-            setlens = calloc(nl, sizeof(int));
-        }
-        MPI_Gather(&setlen, 1, MPI_INT, setlens, 1, MPI_INT, 0, comml);
-
-        int *displs = NULL;
-        if( 0 == rl ) {
-            displs = calloc(nl, sizeof(int));
-            displs[0] = 0;
-            for( i = 1; i < nl; i++ ) {
-                displs[i] = displs[i-1]+setlens[i-1];
+    if( slow_bind_warning ) {
+        MPI_Comm comml = MPI_COMM_NULL; int i, nl = 0, rl = MPI_PROC_NULL;
+        MPI_Comm commw = (MPI_Comm)context->comm_ctx;
+        assert(-1 != context->comm_ctx);
+        MPI_Comm_split_type(commw, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comml);
+        MPI_Comm_size(comml, &nl);
+        if( 1 < nl ) {
+            /* Hu-ho, double check that our binding is not conflicting with other
+             * local procs */
+            MPI_Comm_rank(comml, &rl);
+            char *myset = NULL, *allsets = NULL;
+
+            if( 0 != hwloc_bitmap_list_asprintf(&myset, context->cpuset_allowed_mask) ) {
             }
-            allsets = calloc(displs[nl-1]+setlens[nl-1], sizeof(char));
-        }
-        MPI_Gatherv(myset, setlen, MPI_CHAR, allsets, setlens, displs, MPI_CHAR, 0, comml);
-        free(myset);
-
-        if( 0 == rl ) {
-            int notgood = false;
-            for( i = 1; i < nl; i++ ) {
-                hwloc_bitmap_t other = hwloc_bitmap_alloc();
-                hwloc_bitmap_list_sscanf(other, &allsets[displs[i]]);
-                if(hwloc_bitmap_intersects(context->cpuset_allowed_mask, other)) {
-                    notgood = true;
+            int setlen = strlen(myset);
+            int *setlens = NULL;
+            if( 0 == rl ) {
+                setlens = calloc(nl, sizeof(int));
+            }
+            MPI_Gather(&setlen, 1, MPI_INT, setlens, 1, MPI_INT, 0, comml);
+
+            int *displs = NULL;
+            if( 0 == rl ) {
+                displs = calloc(nl, sizeof(int));
+                displs[0] = 0;
+                for( i = 1; i < nl; i++ ) {
+                    displs[i] = displs[i-1]+setlens[i-1];
                 }
-                hwloc_bitmap_free(other);
+                allsets = calloc(displs[nl-1]+setlens[nl-1], sizeof(char));
             }
-            if( notgood ) {
-                parsec_warning("/!\\ PERFORMANCE MIGHT BE REDUCED /!\\: "
-                               "Multiple PaRSEC processes on the same node may share the same physical core(s);\n"
-                               "\tThis is often unintentional, and will perform poorly.\n"
-                               "\tNote that in managed environments (e.g., ALPS, jsrun), the launcher may set `cgroups`\n"
-                               "\tand hide the real binding from PaRSEC; if you verified that the binding is correct,\n"
-                               "\tthis message can be silenced using the MCA argument `runtime_warn_slow_binding`.\n");
+            MPI_Gatherv(myset, setlen, MPI_CHAR, allsets, setlens, displs, MPI_CHAR, 0, comml);
+            free(myset);
+
+            if( 0 == rl ) {
+                int notgood = false;
+                for( i = 1; i < nl; i++ ) {
+                    hwloc_bitmap_t other = hwloc_bitmap_alloc();
+                    hwloc_bitmap_list_sscanf(other, &allsets[displs[i]]);
+                    if(hwloc_bitmap_intersects(context->cpuset_allowed_mask, other)) {
+                        notgood = true;
+                    }
+                    hwloc_bitmap_free(other);
+                }
+                if( notgood ) {
+                    parsec_warning("/!\\ PERFORMANCE MIGHT BE REDUCED /!\\: "
+                                   "Multiple PaRSEC processes on the same node may share the same physical core(s);\n"
+                                    "\tThis is often unintentional, and will perform poorly.\n"
+                                   "\tNote that in managed environments (e.g., ALPS, jsrun), the launcher may set `cgroups`\n"
+                                   "\tand hide the real binding from PaRSEC; if you verified that the binding is correct,\n"
+                                   "\tthis message can be silenced using the MCA argument `runtime_warn_slow_binding`.\n");
+                }
+                free(setlens);
+                free(allsets);
+                free(displs);
             }
-            free(setlens);
-            free(allsets);
-            free(displs);
         }
     }
     return PARSEC_SUCCESS;
diff --git a/parsec/parsec_internal.h b/parsec/parsec_internal.h
index deb100e3b..691d28e1b 100644
--- a/parsec/parsec_internal.h
+++ b/parsec/parsec_internal.h
@@ -616,6 +616,9 @@ parsec_release_local_OUT_dependencies(parsec_execution_stream_t* es,
                                       parsec_data_copy_t* target_dc,
                                       data_repo_entry_t* target_repo_entry);
 
+int parsec_check_overlapping_binding(parsec_context_t *context);
+int remote_dep_mpi_on(parsec_context_t* context);
+
 #define parsec_execution_context_priority_comparator offsetof(parsec_task_t, priority)
 
 #if defined(PARSEC_SIM)
diff --git a/parsec/parsec_mpi_funnelled.c b/parsec/parsec_mpi_funnelled.c
index f9af87b94..fed940e52 100644
--- a/parsec/parsec_mpi_funnelled.c
+++ b/parsec/parsec_mpi_funnelled.c
@@ -113,7 +113,7 @@ reread:
  * using this structure.
  */
 static int tag_hash_table_size = 1<<MPI_FUNNELLED_MAX_TAG; /**< Default tag hash table size */
-static parsec_hash_table_t *tag_hash_table;
+static parsec_hash_table_t *tag_hash_table = NULL;
 
 static parsec_key_fn_t tag_key_fns = {
     .key_equal = parsec_hash_table_generic_64bits_key_equal,
@@ -182,9 +182,9 @@ static MPI_Comm dep_comm = MPI_COMM_NULL;
 static MPI_Comm dep_self = MPI_COMM_NULL;
 
 static mpi_funnelled_callback_t *array_of_callbacks;
-static MPI_Request           *array_of_requests;
-static int                   *array_of_indices;
-static MPI_Status            *array_of_statuses;
+static MPI_Request              *array_of_requests;
+static int                      *array_of_indices;
+static MPI_Status               *array_of_statuses;
 
 static int size_of_total_reqs = 0;
 static int mpi_funnelled_last_active_req = 0;
@@ -491,6 +491,8 @@ mpi_funnelled_init(parsec_context_t *context)
 
     dep_comm = (MPI_Comm) context->comm_ctx;
 
+    parsec_check_overlapping_binding(context);
+
 #if defined(PARSEC_HAVE_MPI_OVERTAKE)
     if( parsec_param_enable_mpi_overtake ) {
         MPI_Info no_order;
@@ -609,10 +611,10 @@ mpi_funnelled_fini(parsec_comm_engine_t *ce)
     PARSEC_OBJ_DESTRUCT(&mpi_funnelled_dynamic_req_fifo);
 
     parsec_mempool_destruct(mpi_funnelled_mem_reg_handle_mempool);
-    free(mpi_funnelled_mem_reg_handle_mempool);
+    free(mpi_funnelled_mem_reg_handle_mempool); mpi_funnelled_mem_reg_handle_mempool = NULL;
 
     parsec_mempool_destruct(mpi_funnelled_dynamic_req_mempool);
-    free(mpi_funnelled_dynamic_req_mempool);
+    free(mpi_funnelled_dynamic_req_mempool); mpi_funnelled_dynamic_req_mempool = NULL;
 
     /* Remove the static handles */
     MPI_Comm_free(&dep_self); /* dep_self becomes MPI_COMM_NULL */
@@ -622,8 +624,14 @@ mpi_funnelled_fini(parsec_comm_engine_t *ce)
         MPI_Comm_free((MPI_Comm*)&ce->parsec_context->comm_ctx);
         ce->parsec_context->comm_ctx = -1; /* We use -1 for the opaque comm_ctx, rather than the MPI specific MPI_COMM_NULL */
     }
-
+    dep_comm = MPI_COMM_NULL;  /* no communicator */
     MAX_MPI_TAG = -1;  /* mark the layer as uninitialized */
+    size_of_total_reqs = 0;
+    mpi_funnelled_last_active_req = 0;
+    mpi_funnelled_static_req_idx = 0;
+
+    nb_internal_tag = 0;
+    count_internal_tag = 0;
 
     return 1;
 }
diff --git a/parsec/remote_dep_mpi.c b/parsec/remote_dep_mpi.c
index aed70df51..3eb110dc3 100644
--- a/parsec/remote_dep_mpi.c
+++ b/parsec/remote_dep_mpi.c
@@ -42,7 +42,7 @@ static int parsec_param_nb_tasks_extracted = 20;
 static size_t parsec_param_short_limit = RDEP_MSG_SHORT_LIMIT;
 static int parsec_param_enable_aggregate = 0;
 
-parsec_mempool_t *parsec_remote_dep_cb_data_mempool;
+parsec_mempool_t *parsec_remote_dep_cb_data_mempool = NULL;
 
 typedef struct remote_dep_cb_data_s {
     parsec_list_item_t        super;
@@ -166,8 +166,6 @@ static int remote_dep_ce_fini(parsec_context_t* context);
 static int local_dep_nothread_reshape(parsec_execution_stream_t* es,
                                       dep_cmd_item_t *item);
 
-static int remote_dep_mpi_on(parsec_context_t* context);
-
 static int remote_dep_mpi_progress(parsec_execution_stream_t* es);
 
 static void remote_dep_mpi_new_taskpool(parsec_execution_stream_t* es,
@@ -197,8 +195,6 @@ int remote_dep_set_ctx(parsec_context_t* context, intptr_t opaque_comm_ctx )
         return PARSEC_ERROR;
     }
 
-    assert(-1 != opaque_comm_ctx /* -1 reserved for non-initialized */);
-
     if( -1 != context->comm_ctx ) {
 #if 0
         /* Currently, parsec is initialized with comm world.
@@ -218,21 +214,19 @@ int remote_dep_set_ctx(parsec_context_t* context, intptr_t opaque_comm_ctx )
             return PARSEC_SUCCESS;
         }
 #endif
-        MPI_Comm_free((MPI_Comm*)&context->comm_ctx);
+        /* Drop the currently used communicator and all other state of the
+         * communication engine */
+        remote_dep_ce_fini(context);
+        assert( -1 == context->comm_ctx );
     }
     rc = MPI_Comm_dup((MPI_Comm)opaque_comm_ctx, &comm);
     context->comm_ctx = (intptr_t)comm;
-    parsec_taskpool_sync_ids_context(context->comm_ctx);
-
+    /* We need to know who we are and how many others are there, in order to
+     * correctly initialize the communication engine at the next start. */
     MPI_Comm_size( (MPI_Comm)context->comm_ctx, (int*)&(context->nb_nodes));
-    if(context->nb_nodes == 1){
-        /* Corner case when moving from WORLD!=1 to WORLD=1 after parsec_init.
-         * If MPI_COMM_WORLD=1, parsec_init ends up running remote_dep_mpi_on on
-         * the app process, otherwise, communication thread does it.
-         * When moving to WORLD=1, we need to run remote_dep_mpi_on ensure all
-         * MPI is setup as it won't be done later by the comm thread. */
-        remote_dep_mpi_on(context);
-    }
+    MPI_Comm_rank( (MPI_Comm)context->comm_ctx, (int*)&(context->my_rank));
+
+    parsec_taskpool_sync_ids_context(context->comm_ctx);
 
     return (MPI_SUCCESS == rc) ? PARSEC_SUCCESS : PARSEC_ERROR;
 }
@@ -289,14 +283,10 @@ remote_dep_dequeue_init(parsec_context_t* context)
     }
 
     if( -1 == context->comm_ctx ) {
-        MPI_Comm comm;
-        MPI_Comm_dup(MPI_COMM_WORLD, &comm);
-        context->comm_ctx = (intptr_t)comm;
+        MPI_Comm_size( MPI_COMM_WORLD, (int*)&(context->nb_nodes));
+        MPI_Comm_rank( MPI_COMM_WORLD, (int*)&(context->my_rank));
     }
 
-    assert(-1 != context->comm_ctx /* -1 reserved for non-initialized */);
-    MPI_Comm_size( (MPI_Comm)context->comm_ctx, (int*)&(context->nb_nodes));
-
     if(parsec_param_comm_thread_multiple) {
         if( thread_level_support >= MPI_THREAD_MULTIPLE ) {
             context->flags |= PARSEC_CONTEXT_FLAG_COMM_MT;
@@ -320,7 +310,7 @@ remote_dep_dequeue_init(parsec_context_t* context)
     /* From now on the communication capabilities are enabled */
     parsec_communication_engine_up = 1;
     if(context->nb_nodes == 1) {
-        /* We're all by ourselves. In case we need to use comm engin to handle data copies
+        /* We're all by ourselves. In case we need to use the comm engine to handle data copies
          * between different formats let's setup it up.
          */
         remote_dep_ce_init(context);
@@ -361,7 +351,6 @@ int
 remote_dep_dequeue_fini(parsec_context_t* context)
 {
     if( 0 == mpi_initialized ) return 0;
-    (void)context;
 
     /**
      * We suppose the off function was called before. Then we will append a
@@ -474,13 +463,11 @@ remote_dep_mpi_initialize_execution_stream(parsec_context_t *context)
 
 void* remote_dep_dequeue_main(parsec_context_t* context)
 {
-    int whatsup;
+    int whatsup = 0;
 
     remote_dep_bind_thread(context);
     PARSEC_PAPI_SDE_THREAD_INIT();
 
-    remote_dep_ce_init(context);
-
     /* Now synchronize with the main thread */
     pthread_mutex_lock(&mpi_thread_mutex);
     pthread_cond_signal(&mpi_thread_condition);
@@ -491,34 +478,20 @@ void* remote_dep_dequeue_main(parsec_context_t* context)
      * been done before due to the lack of other component initialization.
      */
 
-    /* Let's wait until we are awaken */
-    pthread_cond_wait(&mpi_thread_condition, &mpi_thread_mutex);
-    PARSEC_DEBUG_VERBOSE(20, parsec_comm_output_stream, "MPI: comm engine ON on process %d/%d",
-                         context->my_rank, context->nb_nodes);
-    /* The MPI thread is owning the lock */
-    assert( parsec_communication_engine_up == 2 );
-
-    /* Lazy or delayed initializations */
-    remote_dep_mpi_initialize_execution_stream(context);
-
-    remote_dep_mpi_on(context);
-    /* acknoledge the activation */
-    parsec_communication_engine_up = 3;
-    whatsup = remote_dep_dequeue_nothread_progress(&parsec_comm_es, -1 /* loop till explicitly asked to return */);
-    PARSEC_DEBUG_VERBOSE(20, parsec_comm_output_stream, "MPI: comm engine OFF on process %d/%d",
-                         context->my_rank, context->nb_nodes);
-    parsec_communication_engine_up = 1;  /* went to sleep */
-
     while( -1 != whatsup ) {
         /* Let's wait until we are awaken */
         pthread_cond_wait(&mpi_thread_condition, &mpi_thread_mutex);
+
         PARSEC_DEBUG_VERBOSE(20, parsec_comm_output_stream, "MPI: comm engine ON on process %d/%d",
                              context->my_rank, context->nb_nodes);
+
         /* The MPI thread is owning the lock */
         assert( parsec_communication_engine_up == 2 );
+
         remote_dep_mpi_on(context);
         /* acknowledge the activation */
         parsec_communication_engine_up = 3;
+
         whatsup = remote_dep_dequeue_nothread_progress(&parsec_comm_es, -1 /* loop till explicitly asked to return */);
         PARSEC_DEBUG_VERBOSE(20, parsec_comm_output_stream, "MPI: comm engine OFF on process %d/%d",
                              context->my_rank, context->nb_nodes);
@@ -1298,9 +1271,10 @@ static void remote_dep_mpi_profiling_fini(void)
 #endif  /* PARSEC_PROF_TRACE */
 
 
-static int remote_dep_mpi_on(parsec_context_t* context)
+int remote_dep_mpi_on(parsec_context_t* context)
 {
-    // TODO: make sure this is correct with revamp
+    remote_dep_ce_init(context);
+
 #if defined(PARSEC_PROF_TRACE)
     /* This is less than ideal, but remote_dep_mpi_setup
      * holds a mpi_comm_dup() which is often implemented
@@ -1310,7 +1284,7 @@ static int remote_dep_mpi_on(parsec_context_t* context)
      * a common starting time. */
     parsec_profiling_start();
 #endif
-    (void)context;
+
     return 0;
 }
 
@@ -1375,8 +1349,6 @@ static int remote_dep_mpi_pack_dep(int peer,
         }
 #endif
 
-        // TODO JS: add back short message packing
-
         expected++;
         item->cmd.activate.task.output_mask |= (1U<<k);
         PARSEC_DEBUG_VERBOSE(10, parsec_debug_output, "DATA\t%s\tparam %d\tdeps %p send on demand (increase deps counter by %d [%d])",
@@ -1398,7 +1370,7 @@ static int remote_dep_mpi_pack_dep(int peer,
 }
 
 /**
- * Perform a memcopy with datatypes by doing a local sendrecv.
+ * Perform a memcpy with datatypes by doing a local sendrecv.
  */
 static int remote_dep_nothread_memcpy(parsec_execution_stream_t* es,
                                       dep_cmd_item_t *item)
@@ -2141,6 +2113,11 @@ static int
 remote_dep_ce_init(parsec_context_t* context)
 {
     int rc;
+
+    if( NULL != parsec_remote_dep_cb_data_mempool ) {
+        /* already fully initialized */
+        return 0;
+    }
     /* Do this first to give a chance to the communication engine to define
      * who this process is by setting the corresponding info in the
      * parsec_context.
@@ -2180,6 +2157,8 @@ remote_dep_ce_init(parsec_context_t* context)
                              PARSEC_OBJ_CLASS(remote_dep_cb_data_t), sizeof(remote_dep_cb_data_t),
                              offsetof(remote_dep_cb_data_t, mempool_owner),
                              1);
+    /* Lazy or delayed initializations */
+    remote_dep_mpi_initialize_execution_stream(context);
 
     remote_dep_mpi_profiling_init();
     return 0;
@@ -2196,7 +2175,7 @@ remote_dep_ce_fini(parsec_context_t* context)
     //parsec_ce.tag_unregister(REMOTE_DEP_PUT_END_TAG);
 
     parsec_mempool_destruct(parsec_remote_dep_cb_data_mempool);
-    free(parsec_remote_dep_cb_data_mempool);
+    free(parsec_remote_dep_cb_data_mempool); parsec_remote_dep_cb_data_mempool = NULL;
 
     free(parsec_mpi_same_pos_items); parsec_mpi_same_pos_items = NULL;
     parsec_mpi_same_pos_items_size = 0;
diff --git a/parsec/scheduling.c b/parsec/scheduling.c
index 97eb2ecce..c1fd754b4 100644
--- a/parsec/scheduling.c
+++ b/parsec/scheduling.c
@@ -497,6 +497,15 @@ int __parsec_context_wait( parsec_execution_stream_t* es )
         parsec_barrier_wait( &(parsec_context->barrier) );
         my_barrier_counter = 1;
     } else {
+#if defined(DISTRIBUTED)
+        if( (1 == parsec_communication_engine_up) &&
+            (es->virtual_process[0].parsec_context->nb_nodes == 1) ) {
+            /* If there is a single process run and the main thread is in charge of
+             * progressing the communications we need to make sure the comm engine
+             * is ready for primetime. */
+            remote_dep_mpi_on(parsec_context);
+        }
+#endif /* defined(DISTRIBUTED) */
         /* The master thread might not have to trigger the barrier if the other
          * threads have been activated by a previous start.
          */
@@ -513,7 +522,7 @@ int __parsec_context_wait( parsec_execution_stream_t* es )
 
     /* The main loop where all the threads will spend their time */
   wait_for_the_next_round:
-    /* Wait until all threads are here and the main thread signal the begining of the work */
+    /* Wait until all threads are here and the main thread signal the beginning of the work */
     parsec_barrier_wait( &(parsec_context->barrier) );
 
     if( parsec_context->__parsec_internal_finalization_in_progress ) {
@@ -528,7 +537,6 @@ int __parsec_context_wait( parsec_execution_stream_t* es )
         parsec_fatal("Main thread entered parsec_context_wait, while a scheduler is not selected yet!");
         return -1;
     }
-
   skip_first_barrier:
     while( !all_tasks_done(parsec_context) ) {
 
diff --git a/tests/runtime/multichain.jdf b/tests/runtime/multichain.jdf
index c01b2e3e4..d639f17b5 100644
--- a/tests/runtime/multichain.jdf
+++ b/tests/runtime/multichain.jdf
@@ -27,8 +27,7 @@ static int verbose = 0;
  static int max_comms = 6;
  static MPI_Comm comms[6] = {MPI_COMM_WORLD, MPI_COMM_SELF, MPI_COMM_NULL,
                              MPI_COMM_NULL,  MPI_COMM_NULL, MPI_COMM_WORLD};
-static const char *comm_name[6] = {"MPI_COMM_WORLD", "MPI_COMM_SELF", "MPI_COMM_NULL", 
-                                   "MPI_COMM_NULL", "MPI_COMM_NULL", "MPI_COMM_WORLD"};
+ static char *comm_name[6] = {NULL};
 %}
 
 descA      [type = "parsec_matrix_block_cyclic_t*"]
@@ -74,14 +73,27 @@ static void create_communicators(void)
 
     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
     MPI_Comm_size(MPI_COMM_WORLD, &size);
+
+    comm_name[0] = strdup("MPI_COMM_WORLD");
+    comm_name[1] = strdup("MPI_COMM_SELF");
+
     MPI_Comm_split(MPI_COMM_WORLD, 0, size-1-rank, &comms[2]);
+    asprintf(&comm_name[2], "WORLD.split(0, size-1-rank): same comm reorder ranks");
+
     MPI_Comm_split(MPI_COMM_WORLD, (rank < (size-1) ? 0 : MPI_UNDEFINED), rank, &comms[3]);
+    asprintf(&comm_name[3], "WORLD.split((rank < (size-1) ? 0 : MPI_UNDEFINED), rank): all but last same ranks");
+
     MPI_Comm_split(MPI_COMM_WORLD, rank % 2, rank, &comms[4]);
+    asprintf(&comm_name[4], "WORLD.split(rank %% 2, rank): split comm in 2 same ranks");
+
+    comm_name[5] = strdup("MPI_COMM_WORLD");
 }
+
 static void release_comms(void)
 {
     int i;
     for( i = 0; i < (int)(sizeof(comms) / sizeof(MPI_Comm)); i++ ) {
+        free(comm_name[i]);
         if( (MPI_COMM_WORLD == comms[i]) ||
             (MPI_COMM_SELF == comms[i]) ||
             (MPI_COMM_NULL == comms[i]) )
@@ -111,6 +123,7 @@ int main(int argc, char* argv[])
     parsec_context_t *parsec;
     int ni = NN, nj = NN, loops = 5, sleep_between_comms = 1, i = 1, l, rc;
     int rank = 0, size = 1, mat_size;
+    int world_rank = 0, world_size = 1;
     long time_elapsed;
     parsec_datatype_t baseType, newtype;
     MPI_Comm comm;
@@ -166,6 +179,8 @@ int main(int argc, char* argv[])
     {
         int provided;
         MPI_Init_thread(NULL, NULL, MPI_THREAD_SERIALIZED, &provided);
+        MPI_Comm_size(MPI_COMM_WORLD, &world_size);
+        MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
     }
     create_communicators();
 #endif  /* DISTRIBUTED */
@@ -190,7 +205,8 @@ int main(int argc, char* argv[])
                                    rank /*rank*/,
                                    2*BLOCK, 1, mat_size, 1,
                                    0, 0, mat_size, 1,
-                                   1, size, 1, 1, 0, 0);
+                                   size, 1, 1, 1, 0, 0);
+        printf("Rank %d/%d has %d tiles locally\n", rank, size, descA.super.nb_local_tiles);
         descA.mat = parsec_data_allocate( descA.super.nb_local_tiles *
                                           descA.super.bsiz *
                                           parsec_datadist_getsizeoftype(TYPE) );
@@ -198,7 +214,7 @@ int main(int argc, char* argv[])
                                    rank /*rank*/,
                                    2*BLOCK, 1, mat_size, 1,
                                    0, 0, mat_size, 1,
-                                   1, size, 1, 1, 0, 0);
+                                   size, 1, 1, 1, 0, 0);
         descB.mat = parsec_data_allocate( descB.super.nb_local_tiles *
                                           descB.super.bsiz *
                                           parsec_datadist_getsizeoftype(TYPE) );
@@ -233,7 +249,8 @@ int main(int argc, char* argv[])
             PARSEC_CHECK_ERROR(rc, "parsec_context_wait");
             TIMER_STOP(time_elapsed);
 
-            printf("Comm %s (test %d) Loop %d DAG execution in %ld micro-sec\n", comm_name[i], i, l, time_elapsed);
+            printf("Comm %s (test %d) Loop %d DAG execution in %ld micro-sec [world %d/%d]\n",
+                   comm_name[i], i, l, time_elapsed, world_rank, world_size);
             if( verbose >= 5 ) {
                 printf("<DartMeasurement name=\"no_pri\" type=\"numeric/double\"\n"
                        "                 encoding=\"none\" compression=\"none\">\n"
@@ -268,7 +285,8 @@ int main(int argc, char* argv[])
             PARSEC_CHECK_ERROR(rc, "parsec_context_wait");
             TIMER_STOP(time_elapsed);
 
-            printf("Comm %s (test %d) Loop %d DAG execution in %ld micro-sec\n", comm_name[i], i, l, time_elapsed);
+            printf("Comm %s (test %d) Loop %d DAG execution in %ld micro-sec [world %d/%d]\n",
+                   comm_name[i], i, l, time_elapsed, world_rank, world_size);
         }
         free(descA.mat);
         free(descB.mat);
